# 知识图谱构建

知识图谱是 LightRAG 区别于普通向量 RAG 的核心。本章深入分析实体提取、关系建模、图合并算法的实现细节。

## 文本分块：Token-aware 滑动窗口

在提取实体之前，需要先将长文档切成小块。LightRAG 使用 Token-aware 滑动窗口算法，确保每个块不超过 LLM 的处理能力。

**源码位置**: `lightrag/operate.py:99-162`

```python
def chunking_by_token_size(
    tokenizer: Tokenizer,
    content: str,
    split_by_character: str | None = None,
    split_by_character_only: bool = False,
    chunk_overlap_token_size: int = 100,
    chunk_token_size: int = 1200,
) -> list[dict[str, Any]]:
    tokens = tokenizer.encode(content)

    # 纯 Token 滑动窗口
    for index, start in enumerate(
        range(0, len(tokens), chunk_token_size - chunk_overlap_token_size)
    ):
        chunk_content = tokenizer.decode(tokens[start : start + chunk_token_size])
        results.append({
            "tokens": min(chunk_token_size, len(tokens) - start),
            "content": chunk_content.strip(),
            "chunk_order_index": index,
        })
    return results
```

**算法特点**：

1. **Token 精确控制**：基于 tokenizer 编码，避免字符截断导致的语义破坏。
2. **重叠策略**：默认 100 Token 重叠，��证上下文连续性。
3. **双模式分块**：支持字符分割 + Token 滑动窗口。
4. **容错机制**：`split_by_character_only=False` 时自动处理超长块。

**设计权衡**：

- **块大小**：1200 Token 是经验值，太小会丢失上下文，太大会增加 LLM 成本。
- **重叠大小**：100 Token 保证相邻块有足够的上下文重叠，但会增加存储开销。

## 实体提取：LLM 驱动的结构化抽取

实体提取是知识图谱构建的核心。LightRAG 使用精心设计的 Prompt 引导 LLM 从文本中提取实体和关系。

### Prompt 设计哲学

**源码位置**: `lightrag/prompt.py:11-61`

实体提取 Prompt 的设计体现了"格式即协议"的理念——通过严格的格式约束消除解析歧义。

```python
PROMPTS["entity_extraction_system_prompt"] = """---Role---
You are a Knowledge Graph Specialist responsible for extracting entities and relationships.

---Instructions---
1. **Entity Extraction & Output:**
   - entity_name: Title case, consistent naming
   - entity_type: {entity_types} or "Other"
   - entity_description: Based solely on input text
   - Format: entity{tuple_delimiter}entity_name{tuple_delimiter}entity_type{tuple_delimiter}entity_description

2. **Relationship Extraction & Output:**
   - N-ary decomposition: Alice-Bob-Carol → Alice-Bob, Bob-Carol
   - relationship_keywords: Comma-separated, NO {tuple_delimiter}
   - Format: relation{tuple_delimiter}source{tuple_delimiter}target{tuple_delimiter}keywords{tuple_delimiter}description

3. **Delimiter Protocol:**
   - {tuple_delimiter} = "<|#|>" (atomic marker, NOT fillable)
"""
```

**关键设计决策**：

1. **固定字段数量**：实体 4 字段，关系 5 字段，易于解析。
2. **首字段标识符**：`entity` / `relation` 前缀，类型判断无需额外逻辑。
3. **原子分隔符**：`<|#|>` 是不可填充的标记，消除歧义。
4. **N-ary 关系分解**：强制将多元关系拆成二元关系，符合图数据库的存储模型。

### 反例教学

Prompt 中包含反例教学，避免 LLM 犯常见错误：

```python
# 错误：分隔符内填充内容
entity{tuple_delimiter}Tokyo<|location|>Tokyo is the capital of Japan.

# 正确：分隔符仅作分隔
entity{tuple_delimiter}Tokyo{tuple_delimiter}location{tuple_delimiter}Tokyo is the capital of Japan.
```

这是 **Good Taste** 的体现：通过设计消除特殊情况，而非增加 if/else 判断。

### Multi-gleaning 策略

单次 LLM 调用可能遗漏实体，LightRAG 使用 Multi-gleaning 策略迭代补充。

**源码位置**: `lightrag/operate.py:2814-2943`

```python
async def _process_single_content(chunk_key_dp: tuple[str, TextChunkSchema]):
    # 1. 初次提取
    final_result, timestamp = await use_llm_func_with_cache(
        entity_extraction_user_prompt,
        use_llm_func,
        system_prompt=entity_extraction_system_prompt,
    )

    maybe_nodes, maybe_edges = await _process_extraction_result(final_result, ...)

    # 2. Multi-gleaning: 迭代补充遗漏实体
    for glean_index in range(entity_extract_max_gleaning):
        glean_result, timestamp = await use_llm_func_with_cache(
            entity_continue_extraction_user_prompt,
            use_llm_func,
            history_messages=history,
        )

        glean_nodes, glean_edges = await _process_extraction_result(glean_result, ...)

        if not glean_nodes and not glean_edges:
            break  # 无新实体，提前终止

        maybe_nodes.extend(glean_nodes)
        maybe_edges.extend(glean_edges)
```

**设计意图**：

- **默认 1 次补充**：`entity_extract_max_gleaning=1`，平衡质量和成本。
- **提前终止**：如果补充提取没有新实体，立即停止。
- **历史上下文**：将之前的提取结果作为 history_messages 传入，避免重复提取。

### 并发控制

实体提取涉及大量 LLM 调用，需要精细的并发控制。

```python
chunk_max_async = global_config.get("llm_model_max_async", 4)
semaphore = asyncio.Semaphore(chunk_max_async)

async def _process_with_semaphore(chunk):
    async with semaphore:
        # 检查取消信号
        if pipeline_status.get("cancellation_requested", False):
            raise PipelineCancelledException("User cancelled")

        return await _process_single_content(chunk)

# 批量处理所有 chunks
tasks = [_process_with_semaphore(chunk) for chunk in ordered_chunks]
results = await asyncio.gather(*tasks, return_exceptions=True)
```

**关键设计**：

- **Semaphore 限流**：防止 API 限流，默认 4 个并发。
- **取消支持**：每个 chunk 处理前检查 `cancellation_requested` 标志。
- **容错机制**：`return_exceptions=True` 避免单个 chunk 失败导致全局崩溃。

## 关系建模：N-ary 分解与无向图

### N-ary 关系分解

图数据库天然支持二元关系，多元关系需要分解才能存储。

**示例**：
```
"Alice, Bob, and Carol collaborated on Project X"
→ Alice collaborated with Project X
→ Bob collaborated with Project X
→ Carol collaborated with Project X
```

**Prompt 约束**（`prompt.py:26-27`）：
```
If a single statement describes a relationship involving more than two entities (an N-ary relationship),
decompose it into multiple binary (two-entity) relationship pairs
```

### 无向图设计

LightRAG 默认使用无向图，简化存储和查询。

**Prompt 约束**（`prompt.py:41-43`）：
```
Treat all relationships as **undirected** unless explicitly stated otherwise.
Swapping the source and target entities for an undirected relationship does not constitute a new relationship.
Avoid outputting duplicate relationships.
```

**设计权衡**：

- **优势**：简化存储（只存一条边）、简化查询（无需考虑方向）。
- **劣势**：丢失方向信息（如"A 是 B 的老板"和"B 是 A 的老板"无法区分）。

## 图合并算法

当多个文档提到同一个实体时，需要合并它们的信息。

### 实体合并

**源码位置**: `lightrag/utils_graph.py`

```python
async def merge_nodes_and_edges(
    chunk_results: list[dict],
    knowledge_graph_inst: BaseGraphStorage,
    entity_vdb: BaseVectorStorage,
    ...
):
    # 1. 收集所有实体
    all_entities = {}
    for result in chunk_results:
        for entity in result["entities"]:
            entity_name = entity["entity_name"]
            if entity_name in all_entities:
                # 合并描述
                all_entities[entity_name]["description"] += "\n" + entity["description"]
                all_entities[entity_name]["source_id"].extend(entity["source_id"])
            else:
                all_entities[entity_name] = entity

    # 2. 更新图存储
    for entity_name, entity_data in all_entities.items():
        existing = await knowledge_graph_inst.get_node(entity_name)
        if existing:
            # 合并已有实体
            merged_description = existing["description"] + "\n" + entity_data["description"]
            merged_source_id = list(set(existing["source_id"] + entity_data["source_id"]))
            await knowledge_graph_inst.upsert_node(entity_name, {
                "description": merged_description,
                "source_id": merged_source_id,
            })
        else:
            # 新增实体
            await knowledge_graph_inst.upsert_node(entity_name, entity_data)
```

**合并策略**：

- **描述合并**：用换行符连接多个描述。
- **来源追踪**：`source_id` 记录实体来自哪些 chunk。
- **去重**：`source_id` 使用 set 去重。

### 关系合并

关系合并逻辑类似，但需要处理边的两个端点。

```python
for src_id, tgt_id, edge_data in all_edges:
    # 规范化边的方向（无向图）
    if src_id > tgt_id:
        src_id, tgt_id = tgt_id, src_id

    existing = await knowledge_graph_inst.get_edge(src_id, tgt_id)
    if existing:
        # 合并已有关系
        merged_keywords = list(set(existing["keywords"] + edge_data["keywords"]))
        merged_description = existing["description"] + "\n" + edge_data["description"]
        await knowledge_graph_inst.upsert_edge(src_id, tgt_id, {
            "keywords": merged_keywords,
            "description": merged_description,
        })
    else:
        # 新增关系
        await knowledge_graph_inst.upsert_edge(src_id, tgt_id, edge_data)
```

**关键设计**：

- **方向规范化**：`if src_id > tgt_id: swap`，确保无向边只存一份。
- **关键词去重**：`keywords` 使用 set 去重。

## 增量更新机制

LightRAG 支持增量更新，新文档只需处理新增的 chunk。

### 文档状态追踪

**源码位置**: `lightrag/lightrag.py:1343-1356`

```python
new_docs = {
    id_: {
        "status": DocStatus.PENDING,
        "content_summary": get_content_summary(content),
        "content_length": len(content),
        "created_at": datetime.now(timezone.utc).isoformat(),
        "file_path": file_path,
        "track_id": track_id,
    }
}
```

**状态流转**：
```
PENDING → PROCESSING → COMPLETED
                    ↘ FAILED
```

### 去重机制

**源码位置**: `lightrag/lightrag.py:1362-1404`

```python
# 过滤已存在文档
unique_new_doc_ids = await self.doc_status.filter_keys(all_new_doc_ids)

# 处理重复文档
for doc_id in ignored_ids:
    dup_record_id = compute_mdhash_id(f"{doc_id}-{track_id}", prefix="dup-")
    duplicate_docs[dup_record_id] = {
        "status": DocStatus.FAILED,
        "error_msg": f"Content already exists. Original doc_id: {doc_id}",
        "metadata": {"is_duplicate": True, "original_doc_id": doc_id}
    }
```

**设计意图**：

- **MD5 去重**：通过内容 hash 自动去重。
- **重复记录**：重复文档不是静默忽略，而是记录为 FAILED 状态，方便追踪。

## 删除与重建

删除文档时，需要处理实体和关系的部分删除。

**源码位置**: `lightrag/lightrag.py:3282-3430`

```python
for entity_name in affected_entities:
    entity_data = await self.chunk_entity_relation_graph.get_node(entity_name)
    remaining_source_ids = subtract_source_ids(entity_data["source_id"], chunk_ids)

    if not remaining_source_ids:
        # 完全删除实体
        await self.chunk_entity_relation_graph.delete_node(entity_name)
        await self.entity_chunks.delete([entity_name])
    else:
        # 部分删除 - 重建实体
        remaining_chunks = await self.text_chunks.get_by_ids(remaining_source_ids)
        rebuilt_entity = await rebuild_knowledge_from_chunks(
            entity_name=entity_name,
            chunks=remaining_chunks,
            global_config=asdict(self),
        )
        await self.chunk_entity_relation_graph.upsert_node(entity_name, rebuilt_entity)
```

**删除策略**：

- **完全删除**：实体的所有 source_id 都被删除 → 从图中移除。
- **部分删除**：实体还有其他 source_id → 调用 `rebuild_knowledge_from_chunks()` 重建描述。

这种设计保证了知识图谱的一致性——删除文档后，实体的描述只包含剩余文档的信息。

---

## 章节衔接

**本章回顾**：
- 我们学习了知识图谱构建的完整流程
- 关键收获：Token-aware 分块、Multi-gleaning 提取、N-ary 分解、增量更新

**下一章预告**：
- 在 `03-query.md` 中，我们将学习多模式检索系统
- 为什么需要学习：检索是 RAG 的核心，不同模式适合不同场景
- 关键问题：local/global/hybrid/mix 有什么区别？如何选择？
